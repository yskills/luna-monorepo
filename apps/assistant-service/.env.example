HOST=0.0.0.0
PORT=5050

ASSISTANT_CORS_ORIGINS=http://127.0.0.1:5173,http://localhost:5173
ASSISTANT_API_KEY=

ASSISTANT_MODE_CONFIG_FILE=./config/assistant-mode-config.local.json
ASSISTANT_MEMORY_FILE=./data/assistant-memory.sqlite

# --------------------------------------------
# LLM Profil A (lokal): Ollama
# --------------------------------------------
LLM_PROVIDER=ollama
LLM_MODEL=luna:latest
OLLAMA_HOST=http://127.0.0.1:11434

# --------------------------------------------
# LLM Profil B (Hosting): OpenAI-kompatibel
# Render nutzt typischerweise dieses Profil.
# Für OpenAI:
#   OPENAI_BASE_URL=https://api.openai.com/v1
# Für OpenRouter:
#   OPENAI_BASE_URL=https://openrouter.ai/api/v1
# --------------------------------------------
# LLM_PROVIDER=openai
# LLM_MODEL=gpt-4o-mini
# OPENAI_BASE_URL=https://api.openai.com/v1
# OPENAI_API_KEY=

# Optionaler Fallback-Modelname
# OPENAI_MODEL=gpt-4o-mini

ASSISTANT_FORCE_CHARACTER_ID=luna
ASSISTANT_UNCENSORED_PASSWORD=change-this-now

ASSISTANT_WEB_SEARCH_ENABLED=true
ASSISTANT_WEB_SEARCH_CHARACTERS=luna
ASSISTANT_WEB_SEARCH_MAX_ITEMS=3
ASSISTANT_WEB_SEARCH_TIMEOUT_MS=9000
